{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"nf-cmgg/germline","text":""},{"location":"#introduction","title":"Introduction","text":"<p>nf-cmgg/germline is a nextflow pipeline for calling and annotating small germline variants from short DNA reads for WES and WGS data.</p> <p>The pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from nf-core/modules in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!</p>"},{"location":"#pipeline-summary","title":"Pipeline summary","text":"<p>Note</p> <p>If you are new to Nextflow and nf-core, please refer to this page on how to set-up Nextflow. Make sure to test your setup with <code>-profile test</code> before running the workflow on actual data.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install <code>Nextflow</code> (<code>&gt;=23.10.0</code>)</li> <li>Install any of <code>Docker</code>, <code>Singularity</code> (you can follow this tutorial), <code>Podman</code>, <code>Shifter</code> or <code>Charliecloud</code> for full pipeline reproducibility (you can use <code>Conda</code> both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see docs).</li> </ol> samplesheet.csv<pre><code>sample,family,cram,crai\nSAMPLE_1,FAMILY_1,SAMPLE_1.cram,SAMPLE_1.crai\n</code></pre> <p>Each row represents a single sample to be analysed. More information can be found in the usage documentation.</p> <p>Now, you can run the pipeline using:</p> <pre><code>nextflow run nf-cmgg/germline --input samplesheet.csv --outdir &lt;OUTDIR&gt; --genome GRCh38 -profile &lt;docker/singularity/podman/shifter/charliecloud/conda/institute&gt;\n</code></pre> <p>Warning</p> <p>Please provide pipeline parameters via the CLI or Nextflow <code>-params-file</code> option. Custom config files including those provided by the <code>-c</code> Nextflow option can be used to provide any configuration except for parameters; see docs.</p>"},{"location":"#credits","title":"Credits","text":"<p>nf-cmgg/germline was originally written and is maintained by @nvnieuwk.</p> <p>Special thanks to @matthdsm for the many tips and feedback and to @mvheetve for testing the pipeline.</p>"},{"location":"#contributions-and-support","title":"Contributions and Support","text":"<p>If you would like to contribute to this pipeline, please see the contributing guidelines.</p>"},{"location":"#citations","title":"Citations","text":"<p>An extensive list of references can be found in the <code>CITATIONS</code> section.</p>"},{"location":"CITATIONS/","title":"nf-cmgg/germline: Citations","text":""},{"location":"CITATIONS/#nf-core","title":"nf-core","text":"<p>Ewels PA, Peltzer A, Fillinger S, Patel H, Alneberg J, Wilm A, Garcia MU, Di Tommaso P, Nahnsen S. The nf-core framework for community-curated bioinformatics pipelines. Nat Biotechnol. 2020 Mar;38(3):276-278. doi: 10.1038/s41587-020-0439-x. PubMed PMID: 32055031.</p>"},{"location":"CITATIONS/#nextflow","title":"Nextflow","text":"<p>Di Tommaso P, Chatzou M, Floden EW, Barja PP, Palumbo E, Notredame C. Nextflow enables reproducible computational workflows. Nat Biotechnol. 2017 Apr 11;35(4):316-319. doi: 10.1038/nbt.3820. PubMed PMID: 28398311.</p>"},{"location":"CITATIONS/#software-packagingcontainerisation-tools","title":"Software packaging/containerisation tools","text":"<ul> <li>Anaconda</li> </ul> <p>Anaconda Software Distribution. Computer software. Vers. 2-2.4.0. Anaconda, Nov. 2016. Web.</p> <ul> <li>Bioconda</li> </ul> <p>Gr\u00fcning B, Dale R, Sj\u00f6din A, Chapman BA, Rowe J, Tomkins-Tinch CH, Valieris R, K\u00f6ster J; Bioconda Team. Bioconda: sustainable and comprehensive software distribution for the life sciences. Nat Methods. 2018 Jul;15(7):475-476. doi: 10.1038/s41592-018-0046-7. PubMed PMID: 29967506.</p> <ul> <li>BioContainers</li> </ul> <p>da Veiga Leprevost F, Gr\u00fcning B, Aflitos SA, R\u00f6st HL, Uszkoreit J, Barsnes H, Vaudel M, Moreno P, Gatto L, Weber J, Bai M, Jimenez RC, Sachsenberg T, Pfeuffer J, Alvarez RV, Griss J, Nesvizhskii AI, Perez-Riverol Y. BioContainers: an open-source and community-driven framework for software standardization. Bioinformatics. 2017 Aug 15;33(16):2580-2582. doi: 10.1093/bioinformatics/btx192. PubMed PMID: 28379341; PubMed Central PMCID: PMC5870671.</p> <ul> <li>Docker</li> </ul> <p>Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. doi: 10.5555/2600239.2600241.</p> <ul> <li>Singularity</li> </ul> <p>Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers for mobility of compute. PLoS One. 2017 May 11;12(5):e0177459. doi: 10.1371/journal.pone.0177459. eCollection 2017. PubMed PMID: 28494014; PubMed Central PMCID: PMC5426675.</p>"},{"location":"CITATIONS/#pipeline-tools-in-alphabetical-order","title":"Pipeline tools (in alphabetical order)","text":"<ul> <li>BCFTools</li> </ul> <p>Li H: A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics. 2011 Nov 1;27(21):2987-93. doi: 10.1093/bioinformatics/btr509. PubMed PMID: 21903627; PubMed Central PMCID: PMC3198575.</p> <ul> <li>Bedtools</li> </ul> <p>Quinlan AR and Hall IM, 2010. BEDTools: a flexible suite of utilities for comparing genomic features. Bioinformatics. 26, 6, pp. 841\u2013842.</p> <ul> <li>EnsemblVEP</li> </ul> <p>McLaren W, Gil L, Hunt SE, et al.: The Ensembl Variant Effect Predictor. Genome Biol. 2016 Jun 6;17(1):122. doi: 10.1186/s13059-016-0974-4. PubMed PMID: 27268795; PubMed Central PMCID: PMC4893825.</p> <ul> <li>GATK</li> </ul> <p>McKenna A, Hanna M, Banks E, et al.: The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data. Genome Res. 2010 Sep;20(9):1297-303. doi: 10.1101/gr.107524.110. Epub 2010 Jul 19. PubMed PMID: 20644199; PubMed Central PMCID: PMC2928508.</p> <ul> <li> <p>Gawk</p> </li> <li> <p>GNU tar</p> </li> <li> <p>Mosdepth</p> </li> </ul> <p>Brent S Pedersen, Aaron R Quinlan, Mosdepth: quick coverage calculation for genomes and exomes, Bioinformatics, Volume 34, Issue 5, 01 March 2018, Pages 867\u2013868. doi: 10.1093/bioinformatics/btx699. PubMed PMID: 29096012. PubMed Central PMCID: PMC6030888.</p> <ul> <li>MultiQC</li> </ul> <p>Ewels P, Magnusson M, Lundin S, K\u00e4ller M. MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics. 2016 Oct 1;32(19):3047-8. doi: 10.1093/bioinformatics/btw354. Epub 2016 Jun 16. PubMed PMID: 27312411. PubMed Central PMCID: PMC5039924.</p> <ul> <li> <p>RTG Tools</p> </li> <li> <p>SAMtools</p> </li> </ul> <p>Li H, Handsaker B, Wysoker A, Fennell T, Ruan J, Homer N, Marth G, Abecasis G, Durbin R; 1000 Genome Project Data Processing Subgroup. The Sequence Alignment/Map format and SAMtools. Bioinformatics. 2009 Aug 15;25(16):2078-9. doi: 10.1093/bioinformatics/btp352. Epub 2009 Jun 8. PubMed PMID: 19505943; PubMed Central PMCID: PMC2723002.</p> <ul> <li>Somalier</li> </ul> <p>Lee S, Lee S, Ouellette S, Park WY, Lee EA, Park PJ. NGSCheckMate: software for validating sample identity in next-generation sequencing studies within and across data types. Nucleic Acids Res. 2017 Jun 20;45(11):e103. doi: 10.1093/nar/gkx193. PMID: 28369524; PMCID: PMC5499645.</p> <ul> <li>Tabix</li> </ul> <p>Li H, Tabix: fast retrieval of sequence features from generic TAB-delimited files, Bioinformatics, Volume 27, Issue 5, 1 March 2011, Pages 718\u2013719, doi: 10.1093/bioinformatics/btq671. PubMed PMID: 21208982. PubMed Central PMCID: PMC3042176.</p> <ul> <li>UPDio</li> </ul> <p>King DA, Fitzgerald TW, Miller R, Canham N, Clayton-Smith J, Johnson D, Mansour S, Stewart F, Vasudevan P, Hurles ME; DDD Study. A novel method for detecting uniparental disomy from trio genotypes identifies a significant excess in children with developmental disorders. Genome Res. 2014 Apr;24(4):673-87. doi: 10.1101/gr.160465.113. Epub 2013 Dec 19. PMID: 24356988; PMCID: PMC3975066.</p> <ul> <li>Vardict</li> </ul> <p>Zhongwu Lai, Aleksandra Markovets, Miika Ahdesmaki, Brad Chapman, Oliver Hofmann, Robert McEwen, Justin Johnson, Brian Dougherty, J. Carl Barrett, Jonathan R. Dry, VarDict: a novel and versatile variant caller for next-generation sequencing in cancer research, Nucleic Acids Research, Volume 44, Issue 11, 20 June 2016, Page e108, https://doi.org/10.1093/nar/gkw227</p> <ul> <li> <p>VCF2DB</p> </li> <li> <p>VCFanno</p> </li> </ul> <p>Pedersen, B.S., Layer, R.M. &amp; Quinlan, A.R. Vcfanno: fast, flexible annotation of genetic variants. Genome Biol 17, 118 (2016). https://doi.org/10.1186/s13059-016-0973-5</p>"},{"location":"output/","title":"nf-cmgg/germline: Output","text":""},{"location":"output/#introduction","title":"Introduction","text":"<p>This page describes the output produced by the pipeline.</p> <p>The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level output directory (specified by <code>--outdir &lt;DIR&gt;</code>). This is an example output when the pipeline has been run for a WGS sample called <code>SAMPLE_1</code> and a WES sample called <code>SAMPLE_2</code> which form a family called <code>FAMILY_1</code>. The output consists of 4 directories: <code>yyyy-MM-dd_project_name</code>, <code>individuals</code>, <code>multiqc_reports</code> and <code>pipeline_info</code>. This run has only been run with <code>haplotypecaller</code> (<code>--callers haplotypecaller</code>)</p> <pre><code>results/\n\u251c\u2500\u2500 YYYY_MM_DD_project_name #(1)!\n\u2502   \u2514\u2500\u2500 FAMILY_1 #(2)!\n\u2502       \u251c\u2500\u2500 FAMILY_1.bed #(3)!\n\u2502       \u251c\u2500\u2500 FAMILY_1.haplotypecaller.ped #(4)!\n\u2502       \u251c\u2500\u2500 FAMILY_1.haplotypecaller.vcf.gz #(5)!\n\u2502       \u251c\u2500\u2500 FAMILY_1.haplotypecaller.vcf.gz.tbi #(6)!\n\u2502       \u2514\u2500\u2500 reports\n\u2502           \u251c\u2500\u2500 FAMILY_1.haplotypecaller.bcftools_stats.txt #(7)!\n\u2502           \u2514\u2500\u2500 FAMILY_1.haplotypecaller.somalier.html #(8)!\n\u251c\u2500\u2500 multiqc\n\u2502   \u251c\u2500\u2500 multiqc_data/\n\u2502   \u2514\u2500\u2500 multiqc_report.html #(9)!\n\u251c\u2500\u2500 SAMPLE_1 #(10)!\n\u2502   \u251c\u2500\u2500 SAMPLE_1.bed #(11)!\n\u2502   \u251c\u2500\u2500 SAMPLE_1.haplotypecaller.g.vcf.gz #(12)!\n\u2502   \u251c\u2500\u2500 SAMPLE_1.haplotypecaller.g.vcf.gz.tbi\n\u2502   \u2514\u2500\u2500 reports\n\u2502       \u251c\u2500\u2500 SAMPLE_1.haplotypecaller.bcftools_stats.txt\n\u2502       \u251c\u2500\u2500 SAMPLE_1.mosdepth.global.dist.txt #(13)!\n\u2502       \u2514\u2500\u2500 SAMPLE_1.mosdepth.summary.txt #(14)!\n\u251c\u2500\u2500 SAMPLE_2\n\u2502   \u251c\u2500\u2500 SAMPLE_2.bed\n\u2502   \u251c\u2500\u2500 SAMPLE_2.haplotypecaller.g.vcf.gz\n\u2502   \u251c\u2500\u2500 SAMPLE_2.haplotypecaller.g.vcf.gz.tbi\n\u2502   \u2514\u2500\u2500 reports\n\u2502       \u251c\u2500\u2500 SAMPLE_2.haplotypecaller.bcftools_stats.txt\n\u2502       \u251c\u2500\u2500 SAMPLE_2.mosdepth.global.dist.txt\n\u2502       \u2514\u2500\u2500 SAMPLE_2.mosdepth.summary.txt\n\u251c\u2500\u2500 pipeline_info/ #(15)!\n\u2514\u2500\u2500 samplesheet.csv #(16)!\n</code></pre> <ol> <li> <p>This is the name of the main pipeline output. It contains the current date and the mnemonic name of the pipeline run by default. The date can be excluded with the <code>--skip_date_project</code> parameter and the name can be customized with the <code>--project &lt;STRING&gt;</code> parameter.</p> </li> <li> <p>This directory contains all files for family <code>FAMILY_1</code>.</p> </li> <li> <p>This is the BED file used to parallelize the joint-genotyping. It contains all regions that have reads mapped to them for WGS and all regions in the regions of interest that have reads mapped to them for WES.</p> </li> <li> <p>The PED file detailing the relation between the different members of the family. This file will be inferred when no PED file has been given to this family.</p> </li> <li> <p>The resulting VCF for this family. All desired post-processing has been applied on this file.</p> </li> <li> <p>The index of the resulting VCF.</p> </li> <li> <p>The statistics created with <code>bcftools stats</code> for the resulting VCF.</p> </li> <li> <p>The results of <code>somalier relate</code>.</p> </li> <li> <p>The report created with MultiQC. This contains all statistics generated with <code>bcftools stats</code>, Ensembl VEP and other tools.</p> </li> <li> <p>The folder for <code>SAMPLE_1</code> containing temporary files that could be useful for re-analysing later.</p> </li> <li> <p>This is the BED file used to parallelize the variant calling. It contains all regions that have reads mapped to them for WGS and all regions in the regions of interest that have reads mapped to them for WES.</p> </li> <li> <p>The GVCF file created with <code>haplotypecaller</code>. This can used in later runs of the pipeline to skip variant calling for this sample. A major use case for this is to add a new member to a family without having to call all variants of already called members.</p> </li> <li> <p>The global distribution of the coverage calculated by <code>mosdepth</code>.</p> </li> <li> <p>The summary created by <code>mosdepth</code>.</p> </li> <li> <p>The directory containing information of the pipeline run.</p> </li> <li> <p>The samplesheet used for the pipeline run.</p> </li> </ol>"},{"location":"output/#pipeline-overview","title":"Pipeline overview","text":"<p>Nextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.</p>"},{"location":"parameters/","title":"nf-cmgg/germline pipeline parameters","text":"<p>A nextflow pipeline for calling and annotating variants</p>"},{"location":"parameters/#inputoutput-options","title":"Input/output options","text":"<p>Define where the pipeline should find input data and save output data.</p> Parameter Description Type Default Required Hidden <code>input</code> Path to comma-separated file containing information about the samples in the experiment. HelpYou will need to create a design file with information about the samples in your experiment before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with samples, and a header row. See usage docs. <code>string</code> True <code>outdir</code> The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure. <code>string</code> True <code>email</code> Email address for completion summary. HelpSet this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (<code>~/.nextflow/config</code>) then you don't need to specify this on the command line for every run. <code>string</code>"},{"location":"parameters/#reference-genome-options","title":"Reference genome options","text":"<p>Reference genome related files and options required for the workflow.</p> Parameter Description Type Default Required Hidden <code>genome</code> Reference genome build HelpRequires a Genome Reference Consortium reference ID (e.g. GRCh38) <code>string</code> GRCh38 <code>fasta</code> Path to FASTA genome file. HelpThis parameter is mandatory if <code>--genome</code> is not specified. The path to the reference genome fasta. <code>string</code> True <code>fai</code> Path to FASTA genome index file. <code>string</code> <code>dict</code> Path to the sequence dictionary generated from the FASTA reference <code>string</code> <code>strtablefile</code> Path to the STR table file generated from the FASTA reference <code>string</code> <code>sdf</code> Path to the SDF folder generated from the reference FASTA file <code>string</code> <code>genomes_base</code> Directory base for CMGG reference store (used when --genomes_ignore false is specified) <code>string</code> /references/ <code>cmgg_config_base</code> The base directory for the local config files <code>string</code> /conf/ True <code>genomes_ignore</code> Do not load the local references from the path specified with --genomes_base <code>boolean</code> True <code>igenomes_base</code> Directory / URL base for iGenomes references. <code>string</code> True <code>igenomes_ignore</code> Do not load the iGenomes reference config. HelpDo not load <code>igenomes.config</code> when running the pipeline. You may choose this option if you observe clashes between custom parameters and those supplied in <code>igenomes.config</code>. <code>boolean</code> True True"},{"location":"parameters/#pipeline-specific-parameters","title":"Pipeline specific parameters","text":"<p>Parameters that define how the pipeline works</p> Parameter Description Type Default Required Hidden <code>scatter_count</code> The amount of scattering that should happen per sample. HelpIncrease this number to increase the pipeline run speed, but at the tradeoff of using more IO and disk space. This can differ from the actual scatter count in some cases (especially with smaller files).This has an effect on HaplotypeCaller, GenomicsDBImport and GenotypeGVCFs. <code>integer</code> 40 <code>merge_distance</code> The merge distance for genotype BED files HelpIncrease this parameter if GenomicsDBImport is running slow. This defines the maximum distance between intervals that should be merged. The less intervals GenomicsDBImport actually gets, the faster it will run. <code>integer</code> 100000 <code>dragstr</code> Create DragSTR models to be used with HaplotypeCaller HelpThis currently is only able to run single-core per sample. Due to this, the process is very slow with only very small improvements to the analysis. <code>boolean</code> <code>validate</code> Validate the found variants HelpThis only validates individual sample GVCFs that have truth VCF supplied to them via the samplesheet (in row <code>truth_vcf</code>, with an optional index in the <code>truth_tbi</code> row) <code>boolean</code> <code>filter</code> Filter the found variants <code>boolean</code> <code>annotate</code> Annotate the found variants <code>boolean</code> <code>add_ped</code> Add PED INFO header lines to the final VCFs <code>boolean</code> <code>gemini</code> Create a Gemini databases from the final VCFs <code>boolean</code> <code>mosdepth_slow</code> Don't run mosdepth in fast-mode HelpThis is advised if you need exact coverage BED files as output <code>boolean</code> <code>project</code> The name of the project. HelpThis will be used to specify the final output files folder in the output directory. <code>string</code> <code>skip_date_project</code> Don't add the current date to the output project folder <code>boolean</code> <code>roi</code> Path to the default ROI (regions of interest) BED file to be used for WES analysis HelpThis will be used for all samples that do not have a specific ROI file supplied to them through the samplesheet. Don't supply an ROI file to run the analysis as WGS. <code>string</code> <code>dbsnp</code> Path to the dbSNP VCF file <code>string</code> <code>dbsnp_tbi</code> Path to the index of the dbSNP VCF file <code>string</code> <code>somalier_sites</code> Path to the VCF file with sites for Somalier to use <code>string</code> https://github.com/brentp/somalier/files/3412456/sites.hg38.vcf.gz <code>only_call</code> Only call the variants without doing any post-processing <code>boolean</code> <code>only_merge</code> Only run the pipeline until the creation of the genomicsdbs and output them <code>boolean</code> <code>output_genomicsdb</code> Output the genomicsDB together with the joint-genotyped VCF <code>boolean</code> <code>callers</code> A comma delimited string of the available callers. Current options are: 'haplotypecaller' and 'vardict' <code>string</code> haplotypecaller <code>vardict_min_af</code> The minimum allele frequency for VarDict when no <code>vardict_min_af</code> is supplied in the samplesheet <code>number</code> 0.1 <code>normalize</code> Normalize the VCFs <code>boolean</code> <code>output_suffix</code> A custom suffix to add to the basename of the output files <code>string</code> <code>only_pass</code> Filter out all variants that don't have the PASS filter for vardict. This only works when --filter is also given <code>boolean</code> <code>keep_alt_contigs</code> Keep all aditional contigs for calling instead of filtering them out before <code>boolean</code> <code>updio</code> Run UPDio analysis on the resulting VCFs <code>boolean</code> <code>updio_common_cnvs</code> A TSV file containing common CNVs to be used by UPDio <code>string</code> <code>automap</code> Run AutoMap analysis on the resulting VCFs <code>boolean</code> <code>automap_repeats</code> BED file with repeat regions in the genome. HelpThis file will be automatically generated for hg38/GRCh38 and hg19/GRCh37 when this parameter has not been given. <code>string</code> <code>automap_panel</code> TXT file with gene panel regions to be used by AutoMap. HelpBy default the CMGG gene panel list will be used. <code>string</code> <code>automap_panel_name</code> The panel name of the panel given with --automap_panel. <code>string</code> cmgg_bio"},{"location":"parameters/#institutional-config-options","title":"Institutional config options","text":"<p>Parameters used to describe centralised config profiles. These should not be edited.</p> Parameter Description Type Default Required Hidden <code>custom_config_version</code> Git commit id for Institutional configs. <code>string</code> master True <code>custom_config_base</code> Base directory for Institutional configs. HelpIf you're running offline, Nextflow will not be able to fetch the institutional config files from the internet. If you don't need them, then this is not a problem. If you do need them, you should download the files from the repo and tell Nextflow where to find them with this parameter. <code>string</code> https://raw.githubusercontent.com/nf-core/configs/master True <code>config_profile_name</code> Institutional config name. <code>string</code> True <code>config_profile_description</code> Institutional config description. <code>string</code> True <code>config_profile_contact</code> Institutional config contact information. <code>string</code> True <code>config_profile_url</code> Institutional config URL link. <code>string</code> True"},{"location":"parameters/#max-job-request-options","title":"Max job request options","text":"<p>Set the top limit for requested resources for any single job.</p> Parameter Description Type Default Required Hidden <code>max_cpus</code> Maximum number of CPUs that can be requested for any single job. HelpUse to set an upper-limit for the CPU requirement for each process. Should be an integer e.g. <code>--max_cpus 1</code> <code>integer</code> 16 True <code>max_memory</code> Maximum amount of memory that can be requested for any single job. HelpUse to set an upper-limit for the memory requirement for each process. Should be a string in the format integer-unit e.g. <code>--max_memory '8.GB'</code> <code>string</code> 128.GB True <code>max_time</code> Maximum amount of time that can be requested for any single job. HelpUse to set an upper-limit for the time requirement for each process. Should be a string in the format integer-unit e.g. <code>--max_time '2.h'</code> <code>string</code> 240.h True"},{"location":"parameters/#generic-options","title":"Generic options","text":"<p>Less common options for the pipeline, typically set in a config file.</p> Parameter Description Type Default Required Hidden <code>help</code> Display help text. <code>boolean</code> <code>version</code> Display version and exit. <code>boolean</code> <code>publish_dir_mode</code> Method used to save pipeline results to output directory. HelpThe Nextflow <code>publishDir</code> option specifies which intermediate files should be saved to the output directory. This option tells the pipeline what method should be used to move these files. See Nextflow docs for details. <code>string</code> copy <code>email_on_fail</code> Email address for completion summary, only when pipeline fails. HelpAn email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully. <code>string</code> True <code>plaintext_email</code> Send plain-text email instead of HTML. <code>boolean</code> True <code>max_multiqc_email_size</code> File size limit when attaching MultiQC reports to summary emails. <code>string</code> 25.MB True <code>monochrome_logs</code> Do not use coloured log outputs. <code>boolean</code> True <code>hook_url</code> Incoming hook URL for messaging service HelpIncoming hook URL for messaging service. Currently, MS Teams and Slack are supported. <code>string</code> <code>multiqc_title</code> MultiQC report title. Printed as page header, used for filename if not otherwise specified. <code>string</code> <code>multiqc_config</code> Custom config file to supply to MultiQC. <code>string</code> <code>multiqc_logo</code> Custom logo file to supply to MultiQC. File name must also be set in the MultiQC config file <code>string</code> <code>multiqc_methods_description</code> Custom MultiQC yaml file containing HTML including a methods description. <code>string</code> <code>validate_params</code> Boolean whether to validate parameters against the schema at runtime <code>boolean</code> True True <code>validationShowHiddenParams</code> Show all params when using <code>--help</code> HelpBy default, parameters set as hidden in the schema are not shown on the command line when a user runs with <code>--help</code>. Specifying this option will tell the pipeline to show all parameters. <code>boolean</code> True <code>validationLenientMode</code> Lenient mode for parameter validation <code>boolean</code> True <code>validationFailUnrecognisedParams</code> Fail on unrecognised parameters <code>boolean</code> True <code>validationSchemaIgnoreParams</code> Comma-separated list of parameters to ignore when validating against the schema <code>string</code> genomes,igenomes_base,test_data True"},{"location":"parameters/#annotation-parameters","title":"Annotation parameters","text":"<p>Parameters to configure Ensembl VEP and VCFanno</p> Parameter Description Type Default Required Hidden <code>vep_chunk_size</code> The amount of sites per split VCF as input to VEP <code>integer</code> 50000 <code>species</code> The species of the samples HelpMust be lower case and have underscores as spaces <code>string</code> homo_sapiens <code>vep_merged</code> Specify if the VEP cache is a merged cache <code>boolean</code> True <code>vep_cache</code> The path to the VEP cache <code>string</code> <code>vep_dbnsfp</code> Use the dbNSFP plugin with Ensembl VEP HelpThe '--dbnsfp' and '--dbnsfp_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_spliceai</code> Use the SpliceAI plugin with Ensembl VEP HelpThe '--spliceai_indel', '--spliceai_indel_tbi', '--spliceai_snv' and '--spliceai_snv_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_spliceregion</code> Use the SpliceRegion plugin with Ensembl VEP <code>boolean</code> <code>vep_mastermind</code> Use the Mastermind plugin with Ensembl VEP HelpThe '--mastermind' and '--mastermind_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_maxentscan</code> Use the MaxEntScan plugin with Ensembl VEP HelpThe '--maxentscan' parameter need to be specified when using this parameter. <code>boolean</code> <code>vep_eog</code> Use the custom EOG annotation with Ensembl VEP HelpThe '--eog' and '--eog_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_alphamissense</code> Use the AlphaMissense plugin with Ensembl VEP HelpThe '--alphamissense' and '--alphamissense_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_version</code> The version of the VEP tool to be used <code>string</code> 105 <code>vep_cache_version</code> The version of the VEP cache to be used <code>string</code> 105 <code>dbnsfp</code> Path to the dbSNFP file <code>string</code> <code>dbnsfp_tbi</code> Path to the index of the dbSNFP file <code>string</code> <code>spliceai_indel</code> Path to the VCF containing indels for spliceAI <code>string</code> <code>spliceai_indel_tbi</code> Path to the index of the VCF containing indels for spliceAI <code>string</code> <code>spliceai_snv</code> Path to the VCF containing SNVs for spliceAI <code>string</code> <code>spliceai_snv_tbi</code> Path to the index of the VCF containing SNVs for spliceAI <code>string</code> <code>mastermind</code> Path to the VCF for Mastermind <code>string</code> <code>mastermind_tbi</code> Path to the index of the VCF for Mastermind <code>string</code> <code>alphamissense</code> Path to the TSV for AlphaMissense <code>string</code> <code>alphamissense_tbi</code> Path to the index of the TSV for AlphaMissense <code>string</code> <code>eog</code> Path to the VCF containing EOG annotations <code>string</code> <code>eog_tbi</code> Path to the index of the VCF containing EOG annotations <code>string</code> <code>vcfanno</code> Run annotations with vcfanno <code>boolean</code> <code>vcfanno_config</code> The path to the VCFanno config TOML <code>string</code> <code>vcfanno_lua</code> The path to a Lua script to be used in VCFanno <code>string</code> <code>vcfanno_resources</code> A semicolon-seperated list of resource files for VCFanno, please also supply their indices using this parameter <code>string</code>"},{"location":"usage/","title":"nf-cmgg/germline: Usage","text":"<p>Documentation of pipeline parameters can be found in the parameters documentation</p>"},{"location":"usage/#samplesheet-input","title":"Samplesheet input","text":"<p>You will need to create a samplesheet with information with the samples you would like to analyse before running the pipeline. Use this parameter to specify its location. It can be either a CSV, TSV, JSON or YAML file.</p> <pre><code>--input '[path to samplesheet file]'\n</code></pre>"},{"location":"usage/#example-of-the-samplesheet","title":"Example of the samplesheet","text":"<p>Below is an example of how the samplesheet could look like in the three formats.</p> <p>Note</p> <p>The order and presence of the fields is not set, you can arrange/remove these as you see fit. The only required fields are <code>sample</code> and <code>cram</code>.</p>"},{"location":"usage/#csv","title":"CSV","text":"samplesheet.csv<pre><code>sample,family,cram,crai\nSAMPLE_1,FAMILY_1,SAMPLE_1.cram,SAMPLE_1.crai\nSAMPLE_2,FAMILY_1,SAMPLE_2.cram,SAMPLE_2.crai\nSAMPLE_3,,SAMPLE_3.cram,\n</code></pre>"},{"location":"usage/#tsv","title":"TSV","text":"samplesheet.tsv<pre><code>sample    family    cram   crai\nSAMPLE_1  FAMILY_1  SAMPLE_1.cram SAMPLE_1.crai\nSAMPLE_2  FAMILY_1  SAMPLE_2.cram SAMPLE_2.crai\nSAMPLE_3    SAMPLE_3.cram\n</code></pre>"},{"location":"usage/#yamlyml","title":"YAML/YML","text":"samplesheet.yaml<pre><code>- sample: SAMPLE_1\n  family: FAMILY_1\n  cram: SAMPLE_1.cram\n  crai: SAMPLE_1.crai\n- sample: SAMPLE_2\n  family: FAMILY_1\n  cram: SAMPLE_2.cram\n  crai: SAMPLE_2.crai\n- sample: SAMPLE_3\n  cram: SAMPLE_3.cram\n</code></pre>"},{"location":"usage/#json","title":"JSON","text":"samplesheet.json<pre><code>[\n  {\n    \"sample\": \"SAMPLE_1\",\n    \"family\": \"FAMILY_1\",\n    \"cram\": \"SAMPLE_1.cram\",\n    \"crai\": \"SAMPLE_1.crai\"\n  },\n  {\n    \"sample\": \"SAMPLE_2\",\n    \"family\": \"FAMILY_1\",\n    \"cram\": \"SAMPLE_2.cram\",\n    \"crai\": \"SAMPLE_2.crai\"\n  },\n  {\n    \"sample\": \"SAMPLE_3\",\n    \"cram\": \"SAMPLE_3.cram\"\n  }\n]\n</code></pre>"},{"location":"usage/#full-samplesheet","title":"Full samplesheet","text":"<p>The samplesheet can have following columns:</p> Column Description <code>sample</code> MANDATORY - Custom sample name. This entry has to be identical for multiple sequencing libraries/runs from the same sample. Spaces in sample names are automatically converted to underscores (<code>_</code>). <code>family</code> OPTIONAL - The family ID of the specified sample. This field is optional, as the family id can also be extracted from the <code>ped</code> file. If no <code>ped</code> file and <code>family</code> ID are supplied, the <code>family</code> ID defaults to the <code>sample</code> ID (which means that the resulting VCF will be single-sample). Spaces in family names are automatically converted to underscores (<code>_</code>). <code>cram</code> MANDATORY - Full path to CRAM file to call variants from. File has to have the extension <code>.cram</code> <code>crai</code> OPTIONAL - Full path to CRAM index file. File has to have the extension <code>.crai</code>. <code>ped</code> OPTIONAL - Full path to PED file containing the relational information between samples in the same family. File has to have the extension <code>.ped</code>. <code>truth_vcf</code> OPTIONAL - Full path to the VCF containing all the truth variants of the current sample. The validation subworkflow will be run when this file is supplied and the <code>--validate true</code> flag has been given. File has to have the extension <code>.vcf.gz</code> <code>truth_tbi</code> OPTIONAL - Full path to the index of the truth VCF. This file can either be supplied by the user or generated by the pipeline. File has to have the extensions <code>.tbi</code> <code>roi</code> OPTIONAL - Full path to a BED file containing the regions of interest for the current sample to call on. When this file is given, the pipeline will run this sample in WES mode. (The flag <code>--roi &lt;path&gt;</code> can also be given to run WES mode for all samples using the file specified by the flag) File has to have the extension <code>.bed</code> or <code>.bed.gz</code>. <code>vardict_min_af</code> OPTIONAL - The minimum AF value to use for the vardict variant caller (<code>--callers vardict</code>). This can be set in the samplesheet when it differs for all samples. A default can be set using the <code>--vardict_min_af</code> parameter (whichs defaults to 0.1) <p>Note</p> <p>The <code>sample</code> identifiers have to be the same when you have re-sequenced the same sample more than once e.g. to increase sequencing depth. Either the <code>ped</code> or <code>family</code> field can be used to specify the family name. The pipeline automatically extracts the family id from the <code>ped</code> file if the <code>family</code> field is empty. The <code>family</code> is used to specify on which samples the joint-genotyping should be performed. If neither the <code>ped</code> or <code>family</code> fields are used, the pipeline will default to a single-sample family with the sample name as its ID.</p> <p>This is an example of a working samplesheet used to test this pipeline:</p> samplesheet.csv<pre><code>sample,family,cram,crai,roi,ped,truth_vcf,truth_tbi,truth_bed,vardict_min_af\nNA24143,Proband_12345,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/crams/NA24143.cram,,,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/genome/test.ped,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/vcfs/NA24143.vcf.gz,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/vcfs/NA24143.vcf.gz.tbi,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/regions/roi.bed,0.01\nNA24149,Proband_12345,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/crams/NA24149.cram,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/crams/NA24149.cram.crai,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/regions/roi.bed,,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/vcfs/NA24149.vcf.gz,,,\nNA24385,Proband_12345,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/crams/NA24385.cram,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/crams/NA24385.cram.crai,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/regions/roi.bed,,https://github.com/nf-cmgg/test-datasets/raw/germline/data/genomics/homo_sapiens/illumina/vcfs/NA24385.vcf.gz,,,\n</code></pre>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:</p> <pre><code>nextflow run nf-cmgg/germline --input ./samplesheet.csv --outdir ./results --genome GRCh38 -profile docker\n</code></pre> <p>This will launch the pipeline with the <code>docker</code> configuration profile. See below for more information about profiles.</p> <p>Note that the pipeline will create the following files in your working directory:</p> <pre><code>work          #(1)!\nresults       #(2)!\n.nextflow_log #(3)!\n...           #(4)!\n</code></pre> <ol> <li> <p>Directory containing the nextflow working files</p> </li> <li> <p>Finished results in specified location (defined with --outdir)</p> </li> <li> <p>Log file from Nextflow</p> </li> <li> <p>Other nextflow hidden files, eg. history of pipeline runs and old logs.</p> </li> </ol> <p>If you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.</p> <p>Pipeline settings can be provided in a <code>yaml</code> or <code>json</code> file via <code>-params-file &lt;file&gt;</code>.</p> <p>Warning</p> <p>Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> <p>The above pipeline run specified with a params file in yaml format:</p> <pre><code>nextflow run nf-cmgg/germline -profile docker -params-file params.yaml\n</code></pre> <p>with <code>params.yaml</code> containing:</p> params.yaml<pre><code>input: './samplesheet.csv'\noutdir: './results/'\ngenome: 'GRCh38'\n&lt;...&gt;\n</code></pre>"},{"location":"usage/#updating-the-pipeline","title":"Updating the pipeline","text":"<p>When you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you're running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline. You can also add the <code>-latest</code> argument to your run command to automatically fetch the latest version on every run:</p> <pre><code>nextflow pull nf-cmgg/germline\n</code></pre>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>It is a good idea to specify a pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you'll be running the same version of the pipeline, even if there have been changes to the code since.</p> <p>First, go to the nf-cmgg/germline releases page and find the latest pipeline version - numeric only (eg. <code>1.3.1</code>). Then specify this when running the pipeline with <code>-r</code> (one hyphen) - eg. <code>-r 1.3.1</code>. Of course, you can switch to another version by changing the number after the <code>-r</code> flag.</p> <p>This version number will be logged in reports when you run the pipeline, so that you'll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.</p> <p>To further assist in reproducbility, you can use share and re-use parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.</p> <p>Tip</p> <p>If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.</p>"},{"location":"usage/#core-nextflow-arguments","title":"Core Nextflow arguments","text":"<p>Note</p> <p>These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen).</p>"},{"location":"usage/#-profile","title":"<code>-profile</code>","text":"<p>Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.</p> <p>Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.</p> <p>Info</p> <p>We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.</p> <p>The pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to see if your system is available in these configs please see the nf-core/configs documentation.</p> <p>Note that multiple profiles can be loaded, for example: <code>-profile test,docker</code> - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>. This is not recommended, since it can lead to different results on different machines dependent on the computer enviroment.</p> <ul> <li><code>test</code> <p>A profile with a complete configuration for automated testing Includes links to test data so needs no other parameters</p> </li> <li><code>nf_test</code> <p>The profile setting the default values for <code>nf-test</code>. When running <code>nf-test</code> this profile is automatically used.</p> </li> <li><code>docker</code> <p>A generic configuration profile to be used with Docker</p> </li> <li><code>singularity</code> <p>A generic configuration profile to be used with Singularity</p> </li> <li><code>podman</code> <p>A generic configuration profile to be used with Podman</p> </li> <li><code>shifter</code> <p>A generic configuration profile to be used with Shifter</p> </li> <li><code>charliecloud</code> <p>A generic configuration profile to be used with Charliecloud</p> </li> <li><code>apptainer</code> <p>A generic configuration profile to be used with Apptainer</p> </li> <li><code>conda</code> <p>A generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it's not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.</p> </li> </ul>"},{"location":"usage/#-resume","title":"<code>-resume</code>","text":"<p>Specify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files' contents as well. For more info about this parameter, see this blog post.</p> <p>You can also supply a run name to resume a specific run: <code>-resume [run-name]</code>. Use the <code>nextflow log</code> command to show previous run names.</p>"},{"location":"usage/#-c","title":"<code>-c</code>","text":"<p>Specify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information.</p>"},{"location":"usage/#custom-configuration","title":"Custom configuration","text":""},{"location":"usage/#resource-requests","title":"Resource requests","text":"<p>Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customise the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the steps in the pipeline, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher requests (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.</p> <p>To change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.</p>"},{"location":"usage/#custom-containers","title":"Custom Containers","text":"<p>In some cases you may wish to change which container or conda environment a step of the pipeline uses for a particular tool. By default nf-core pipelines use containers and software from the biocontainers or bioconda projects. However in some cases the pipeline specified version maybe out of date.</p> <p>To use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.</p>"},{"location":"usage/#custom-tool-arguments","title":"Custom Tool Arguments","text":"<p>A pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, nf-core pipelines provide some freedom to users to insert additional parameters that the pipeline does not include by default.</p> <p>To learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.</p>"},{"location":"usage/#nf-coreconfigs","title":"nf-core/configs","text":"<p>In most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running nf-core pipelines regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the <code>nf-core/configs</code> git repository. Before you do this please can you test that the config file works with your pipeline of choice using the <code>-c</code> parameter. You can then create a pull request to the <code>nf-core/configs</code> repository with the addition of your config file, associated documentation file (see examples in <code>nf-core/configs/docs</code>), and amending <code>nfcore_custom.config</code> to include your custom profile.</p> <p>See the main Nextflow documentation for more information about creating your own configuration files.</p> <p>If you have any questions or issues please send us a message on Slack on the <code>#configs</code> channel.</p>"},{"location":"usage/#azure-resource-requests","title":"Azure Resource Requests","text":"<p>To be used with the <code>azurebatch</code> profile by specifying the <code>-profile azurebatch</code>. We recommend providing a compute <code>params.vm_type</code> of <code>Standard_D16_v3</code> VMs by default but these options can be changed if required.</p> <p>Note that the choice of VM size depends on your quota and the overall workload during the analysis. For a thorough list, please refer the Azure Sizes for virtual machines in Azure.</p>"},{"location":"usage/#running-in-the-background","title":"Running in the background","text":"<p>Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.</p> <p>The Nextflow <code>-bg</code> flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.</p> <p>Alternatively, you can use <code>screen</code> / <code>tmux</code> or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).</p>"},{"location":"usage/#nextflow-memory-requirements","title":"Nextflow memory requirements","text":"<p>In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in <code>~/.bashrc</code> or <code>~./bash_profile</code>):</p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"}]}